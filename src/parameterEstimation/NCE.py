import torch
import numpy as np
from torch import optim

class NCE(torch.nn.Module):

    def __init__(self, num_epochs):
        super(NCE, self).__init__()
        self.num_epochs = num_epochs
        # should I define anything here?


    def noise(self, data):
        """ This function should be a distribution we can draw x number of samples from"""
        # Right now just a uniform distribution
        return np.random.uniform(0, 1, size=len(data))

    def NCE_TG(self, data, num_epochs):
        """This function is used to estimate the parameter theta for the torus graph model.
        Look at equation 2 and 4 in Matsuda19. 
        
        Args:
        data: The data samples, either drawn from synthetic or real data
        num_epochs: The number of epochs to train the model

        Returns:
        The estimated theta for the torus graph model. Theta is intialized to 1
        
        """
        theta = torch.tensor(1.0, requires_grad=True)
        optimizer = optim.SGD([theta], lr=0.01)

        for epoch in range(num_epochs):
            # Think about if noise samples should be generated by data or data_samples
            data_samples = torch.tensor(np.random.choice(data, size=100), dtype=torch.float32)
            noise_samples = torch.tensor(np.random.choice(self.noise(data), size=100), dtype=torch.float32)
            # maybe noise(data) should be noise(num_samples) depedning on the noise function

            # compite the probabiltiy of the data samples. So the pdf
            # This is the probability of the data sample being generated by the TG given theta
            data_prob = torch.exp(data_samples * theta)
            noise_prob = torch.exp(noise_samples * theta)

            nce_loss = -torch.log(data_prob).mean() + torch.log(noise_prob).mean()
            # Should this be the positive log likelihood?

            nce_loss.backward() # Compute the gradient of the loss
            optimizer.step() # Update the parameters 
            optimizer.zero_grad() # Zero the gradien


        return theta # maybe theta.item() to get the value of theta
    
    def NCE_TGMM(self, data, num_epochs): # This is the log exp for the multible torus graph hence a sum.
        """This function is used estimate the parameters theta and pi for the torus graph mixture model.
        Args:
            data: The data samples, either drawn from synthetic or real data
            num_epochs: The number of epochs to train the model
        
        Returns:
            The estimated theta for the torus graphs.
            The estimated pi for the mixture model.
        """
        # M is the number of torus graphs
        M = data.shape[0]
        theta = torch.tensor([1.0]*M, requires_grad=True)
        pi = torch.tensor([1.0]*M, requires_grad=True)
        optimizer = optim.SGD([theta, pi], lr=0.01)

        for epoch in range(num_epochs):
            data_samples = torch.tensor(np.random.choice(data, size=100), dtype=torch.float32)
            noise_samples = torch.tensor(np.random.choice(self.noise(data), size=100), dtype=torch.float32)

            #logprob of each torus graph model
            components_data = data_samples[None, :] * theta[:, None]
            components_noise = noise_samples[None, :] * theta[:, None]

            log_data_prob = torch.logsumexp(components_data + torch.log_softmax(pi, dim=0)[:, None], dim=0)
            log_noise_prob = torch.logsumexp(components_noise + torch.log_softmax(pi, dim=0)[:, None], dim=0)

            nce_loss = -log_data_prob.mean() + log_noise_prob.mean()
            nce_loss.backward()
            optimizer.step()
            optimizer.zero_grad()

        return theta, pi
