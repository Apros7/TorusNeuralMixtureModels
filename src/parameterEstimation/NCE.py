import torch
import numpy as np
from torch import optim

class NCE(torch.nn.Module):

    def __init__(self, num_epochs, p, M):
        super(NCE, self).__init__()
        """ 
        Descrip:

        Args:
        p: The number of different signals in the data. Fx different brain areas.
        M: The number of mixture components in the model. For TG, M = 1.
        num_epochs: The number of epochs to train the model.

        """
        self.p = p
        self.M = M
        self.num_epochs = num_epochs
        self.theta = torch.rand(2,2,(p*(p-1)/2), requires_grad=True) # TODO:skal det være 2,2,somehting eller noget andet?
        # rand(number of theta = 2, number_signals = 2, number_of_thetas = p*(p-1)/2)
        self.theta1 = theta[0]
        self.theta2 = theta[1]
        self.c = torch.nn.parameter.Parameter(torch.tensor(1.0, requires_grad=True)*torch.ones(M))

        #dimensioner
        # theta, pi
        # theta: symmetrisk matrix. Initilize theta som øvre/nedre triangular matrix og projicer ind på matrix
            # torch.triu_indices
            # vejktoren: p(p-1)/2 længden
            # i loss: theta[torch.triu_indices] = theta_vec
            # 2 styks theta-vec for j og k.
         # c til TGMM, skalar dimension: K antal TG, til alm TG kun et skalar.
            # proportionalitet indebærer 1/c. 
            


    def noise(self, data):
        """ This function should be a distribution we can draw x number of samples from"""
        # Right now just a uniform distribution
        # TODO: Ask Morten
        return np.random.uniform(0, 1, size=len(data))

    def NCE_TG(self, data, num_epochs):
        """This function is used to estimate the parameter theta for the torus graph model.
        Look at equation 2 and 4 in Matsuda19. 
        
        Args:
        data: The data samples, either drawn from synthetic or real data
        num_epochs: The number of epochs to train the model

        Returns:
        The estimated theta for the torus graph model and the constant c.
        
        """
        optimizer = optim.SGD([self.theta,self.c], lr=0.01)

        for epoch in range(num_epochs):
            # TODO: Think about if noise samples should be generated by data or data_samples
            data_samples = torch.tensor(np.random.choice(data, size=100), dtype=torch.float32)
            noise_samples = torch.tensor(np.random.choice(self.noise(data), size=100), dtype=torch.float32)
            # maybe noise(data) should be noise(num_samples)=noise(len(data_samples)) depedning on the noise function

         
            # TODO: skal der tages cos og sin af datasamples og så ganges med theta som i en rigtig TG eller 
            # er det allerede gjort når data_samples er genereret?

            # To get the log_prop_data and log_prop_noise, the data has to go through the TG model.
            # TODO: Therefore I implemented the TG model below, but perhaps it could be replaced when the TG model is made.
            # p = j: will be number of rows in data_samples, ie different signals.
            diag = torch.diag(self.theta)
            log_prop_data = []
            log_prop_noise = []
            for j,row in enumerate(data_samples):
                for k,value in enumerate(row):
                    log_prop_data.append(1/self.c + [self.theta[0][j][k],self.theta[1][j][k]] * np.array([np.cos(value-diag), np.sin(value-diag)]))
                    # can be replaced with self.theta1[j][k] and self.theta2[j][k]

            # TODO: value - diag? det skal være cos(x_j - x_k), men hvad betyder det og er det det?

            for j,row in enumerate(noise_samples):
                for k,value in enumerate(row):
                    log_prop_data.append(1/self.c + [self.theta[0][j][k],self.theta[1][j][k]] * np.array([np.cos(value-diag), np.sin(value-diag)]))
            
            
            nce_loss = -np.mean(log_prop_data) + np.mean(log_prop_noise)

            nce_loss.backward() # Gradient of the loss
            optimizer.step() # Update parameters 
            optimizer.zero_grad() # Zero the gradien


        return self.theta,self.c # maybe theta.item() to get the value
    
    # TODO: update the code to match the TG
    def NCE_TGMM(self, data, num_epochs): # This is the log exp for the multible torus graph hence a sum.
        # pi_k * log_prop_data_k, where k is the specfic TG
        # TODO: maybe just use NCE_TG and then sum over the different TGs
        """This function is used estimate the parameters theta and pi for the torus graph mixture model.
        Args:
            data: The data samples, either drawn from synthetic or real data
            num_epochs: The number of epochs to train the model
        
        Returns:
            The estimated theta for the torus graphs.
            The estimated pi for the mixture model.
        """
        # M is the number of torus graphs
        M = data.shape[0]
        theta = torch.tensor([1.0]*M, requires_grad=True)
        pi = torch.tensor([1.0]*M, requires_grad=True)
        optimizer = optim.SGD([theta, pi], lr=0.01)

        for epoch in range(num_epochs):
            data_samples = torch.tensor(np.random.choice(data, size=100), dtype=torch.float32)
            noise_samples = torch.tensor(np.random.choice(self.noise(data), size=100), dtype=torch.float32)

            #logprob of each torus graph model
            components_data = data_samples[None, :] * theta[:, None]
            components_noise = noise_samples[None, :] * theta[:, None]

            log_data_prob = torch.logsumexp(components_data + torch.log_softmax(pi, dim=0)[:, None], dim=0)
            log_noise_prob = torch.logsumexp(components_noise + torch.log_softmax(pi, dim=0)[:, None], dim=0)

            nce_loss = -log_data_prob.mean() + log_noise_prob.mean()
            nce_loss.backward()
            optimizer.step()
            optimizer.zero_grad()

        return theta, pi
    

# Below is just a random test of the code, should be removed
if __name__ == "__main__":
    data = np.random.uniform(0, 1, size=1000)
    nce = NCE(100)
    theta = nce.NCE_TG(data, 100)
    print("Parameters for TG",theta)
    data = np.random.uniform(0, 1, size=(2, 1000))
    theta, pi = nce.NCE_TGMM(data, 100)
    print("Parameters for TGMM",theta, pi)



# M: antal noise samples
## LOSS, J_NCE = 
# 1/N sum_over_datapunkter log (N * log_prop_data/(N*log_prop_data + M*noise))
# 1/N sum_over_noise log ((M*noise)/ N*log_prop_data + M*noise)
# plot loss kurve


# p(x|theta,c) = log_prop_data = 1/c + sum(trekantsmatrix)

# sammenlign med Klein resultater (også i resultatsafsnit, også om SM og NCE giver det samme på syntetisk data.)

# Resultat indehode boxplot over log_ptilde på y-aksen hvor SM og NCE er på x-aksen.